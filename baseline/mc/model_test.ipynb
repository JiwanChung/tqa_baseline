{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from models import TextModel, ModuleNet\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from tensorboard import Logger\n",
    "from utils.ReCuda import ReCuda\n",
    "\n",
    "logger = Logger('./logs')\n",
    "\n",
    "args = type('args_test', (object,), {})()\n",
    "args.load_ckpt = None\n",
    "args.train = True\n",
    "args.test = False\n",
    "args.source_dir = '/home/jiwan/tqa/prepro/data'\n",
    "args.ckpt_dir = './ckpt'\n",
    "args.emb_dim = 300\n",
    "args.repeat = False\n",
    "args.learning_rate = 0.001\n",
    "args.if_pair = False\n",
    "args.log_epoch = 4\n",
    "args.bi_gru = True\n",
    "args.batch_size = 36\n",
    "args.verbose = False\n",
    "args.end_epoch = 100\n",
    "args.single_topic = False\n",
    "args.embed_size = 100\n",
    "args.shuffle = True\n",
    "args.large_topic = False\n",
    "args.reversible = True\n",
    "args.fix_length = True\n",
    "args.reasoning_planes = 16\n",
    "args.k = 4\n",
    "args.conf = 0.7\n",
    "args.h_size = 128\n",
    "args.hyper = False\n",
    "args.hidden_size = 300\n",
    "args.dim_words = 2\n",
    "args.ans_k = 7\n",
    "args.bi = 2 if args.bi_gru else 1\n",
    "\n",
    "args.resume = False\n",
    "if args.load_ckpt is not None:\n",
    "    args.resume = True\n",
    "\n",
    "args.test_iter = 'val'\n",
    "\n",
    "args.cuda = True\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "config = args\n",
    "config.recuda = ReCuda(config)\n",
    "config.ckpt_name = '_single'\n",
    "if not config.single_topic:\n",
    "    config.ckpt_name = '_all'\n",
    "if config.large_topic:\n",
    "    config.ckpt_name = '_full'\n",
    "\n",
    "config.logger = logger\n",
    "\n",
    "config.recuda.torch.manual_seed(1)\n",
    "\n",
    "config.model = ModuleNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data_train_full.tsv, data_test_full.tsv, data_val_full.tsv\n"
     ]
    }
   ],
   "source": [
    "from readData import get_data\n",
    "\n",
    "data, iters, vocab, stats = get_data(config)\n",
    "\n",
    "config.q_size = stats['question_size']\n",
    "config.a_size = stats['answer_size']\n",
    "config.c_size = stats['topic_size']\n",
    "config.keys = ['A', 'c']\n",
    "config.sizes = {'A': config.a_size, 'c': config.c_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PARAMS: ', <bound method ModuleNet.parameters of ModuleNet(\n",
      "  (encoder): Encoder(\n",
      "    (embed): Embedding(27818, 300)\n",
      "    (embed_context): GRU(300, 100, bidirectional=True)\n",
      "    (embed_question): GRU(300, 100, bidirectional=True)\n",
      "    (embed_answer): GRU(300, 100, bidirectional=True)\n",
      "    (normalize_row): Softmax()\n",
      "  )\n",
      "  (controller): POCController(\n",
      "    (module): SimpleModule(\n",
      "      (memory_attention): MemoryAttention(\n",
      "      )\n",
      "      (reasoning): Reasoning(\n",
      "        (res_conv_a): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "        )\n",
      "        (res_conv_x): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "        )\n",
      "        (bm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (question_attend): QuestionAttend(\n",
      "        (res_conv_x): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (2): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): Linear(in_features=128, out_features=34, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (forget_gate): ForgetGate(\n",
      "        (res_conv_h): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (3): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (4): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (5): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (downsample): DownSampleH(\n",
      "          (downsample): Sequential(\n",
      "            (0): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (1): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (2): Flatten(\n",
      "            )\n",
      "            (3): Linear(in_features=6200, out_features=1, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (confidence): Confidence(\n",
      "        (downsample): DownSampleH(\n",
      "          (downsample): Sequential(\n",
      "            (0): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (1): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (2): Flatten(\n",
      "            )\n",
      "            (3): Linear(in_features=6200, out_features=1, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (output): Output(\n",
      "        (res_conv_o): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (3): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (4): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (5): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (sampledown): Sequential(\n",
      "      (0): Linear(in_features=896, out_features=300, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=300, out_features=7, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      ")>)\n"
     ]
    }
   ],
   "source": [
    "def get_net(config, vocab):\n",
    "    if config.resume:\n",
    "        assert os.path.isdir('ckpt'), 'Error: no dir'\n",
    "        ckpt = torch.load(os.path.join(config.ckpt_dir, config.load_ckpt))\n",
    "        net = config.model(config, vocab)\n",
    "        net.load_state_dict(ckpt['params'])\n",
    "        best_acc = ckpt['acc']\n",
    "        start_epoch = ckpt['epoch']\n",
    "        print('RESUME {}th epoch'.format(start_epoch))\n",
    "    else:\n",
    "        net = config.model(config, vocab)\n",
    "        best_acc = 0\n",
    "        start_epoch = 0\n",
    "    net = config.recuda.var(net)\n",
    "    print('PARAMS: ', net.parameters)\n",
    "    return net, best_acc, start_epoch\n",
    "\n",
    "net, best_acc, config.start_epoch = get_net(config, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "##\n",
    "def run_net(net, config, data):\n",
    "    answers_size = len(data.answers)\n",
    "    answers = torch.stack(data.answers, dim=2)\n",
    "\n",
    "    if config.single_topic:\n",
    "        topics = data.topic.data\n",
    "    else:\n",
    "        topics = torch.stack(data.topic, dim=2)\n",
    "\n",
    "    target = Variable(data.correct_answer.data, requires_grad=False)\n",
    "    target = config.recuda.var(target)\n",
    "    # print('t:', topics.size(), type(topics))\n",
    "    # run\n",
    "    return net.forward(topics, data.question, answers)\n",
    "\n",
    "\n",
    "##\n",
    "def train_epoch(net, config, data, train_iter, epoch):\n",
    "\n",
    "    # train\n",
    "    train_loss = 0\n",
    "    for batch_index, data in tqdm(enumerate(train_iter)):\n",
    "        net.zero_grad()\n",
    "        target = Variable(data.correct_answer.data, requires_grad=False)\n",
    "        target = config.recuda.var(target)\n",
    "\n",
    "        if config.verbose:\n",
    "            if config.single_topic:\n",
    "                print('context:', data.topic.data)\n",
    "            else:\n",
    "                print('context_list:', data.topic[0])\n",
    "\n",
    "        # run\n",
    "        y = run_net(net, config, data)\n",
    "\n",
    "        if config.verbose:\n",
    "            print('y:', y.data)\n",
    "            print('t:', target.data)\n",
    "\n",
    "        loss = config.loss_fn(y, target)\n",
    "        # count loss\n",
    "        loss.backward()\n",
    "        # optimize\n",
    "        config.optimizer.step()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        loss_per = train_loss / (batch_index + 1)\n",
    "        print(\"Training {} epoch, loss: {}\".format(epoch, loss_per))\n",
    "        config.logger.scalar_summary('tr_loss{}'.format(config.ckpt_name), loss_per, epoch + 1)\n",
    "\n",
    "\n",
    "##\n",
    "def validate_epoch(net, config, data, val_iter, epoch):\n",
    "    # validate from time to time\n",
    "\n",
    "    print(\"begin validation\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for index_v, data in tqdm(enumerate(val_iter)):\n",
    "        # run\n",
    "        y = run_net(net, config, data)\n",
    "\n",
    "        value, pred = torch.max(y, 1)\n",
    "        check = torch.eq(data.correct_answer.data, pred.data)\n",
    "        if config.verbose:\n",
    "            print(torch.sum(check), check.size())\n",
    "        correct += torch.sum(check)\n",
    "        total += (check.size()[0])\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    print(\"Val {} epoch, acc: {}\".format(epoch, acc))\n",
    "\n",
    "    config.logger.scalar_summary('val_acc{}'.format(config.ckpt_name), acc, (epoch + 1))\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "##\n",
    "def save_net(net, config, epoch, acc):\n",
    "    print('saving')\n",
    "    state = {\n",
    "        'params': net.state_dict(),\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    if not os.path.isdir('ckpt'):\n",
    "        os.mkdir('ckpt')\n",
    "    if not os.path.isdir('ckpt/temp'):\n",
    "        os.mkdir('ckpt/temp')\n",
    "    torch.save(state, os.path.join(config.ckpt_dir, 'temp', 'ckpt{}_{}.t7'.format(config.ckpt_name,epoch)))\n",
    "\n",
    "\n",
    "##\n",
    "def train_all(net, data, iters, config):\n",
    "    config.loss_fn = nn.CrossEntropyLoss()\n",
    "    config.optimizer = torch.optim.Adam(net.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    for epoch in range(config.start_epoch, config.end_epoch):\n",
    "        print(\"{} epoch\".format(epoch))\n",
    "        train_epoch(net, config, data, iters['train'], epoch)\n",
    "        acc = validate_epoch(net, config, data, iters['val'], epoch)\n",
    "\n",
    "        save_net(net, config, epoch, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PARAMS: ', <bound method ModuleNet.parameters of ModuleNet(\n",
      "  (encoder): Encoder(\n",
      "    (embed): Embedding(27818, 300)\n",
      "    (embed_context): GRU(300, 100, bidirectional=True)\n",
      "    (embed_question): GRU(300, 100, bidirectional=True)\n",
      "    (embed_answer): GRU(300, 100, bidirectional=True)\n",
      "    (normalize_row): Softmax()\n",
      "  )\n",
      "  (controller): POCController(\n",
      "    (module): SimpleModule(\n",
      "      (memory_attention): MemoryAttention(\n",
      "      )\n",
      "      (reasoning): Reasoning(\n",
      "        (res_conv_a): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "        )\n",
      "        (res_conv_x): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "        )\n",
      "        (bm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (question_attend): QuestionAttend(\n",
      "        (res_conv_x): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (2): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): Linear(in_features=128, out_features=34, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (forget_gate): ForgetGate(\n",
      "        (res_conv_h): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (3): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (4): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (5): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (downsample): DownSampleH(\n",
      "          (downsample): Sequential(\n",
      "            (0): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (1): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (2): Flatten(\n",
      "            )\n",
      "            (3): Linear(in_features=6200, out_features=1, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (confidence): Confidence(\n",
      "        (downsample): DownSampleH(\n",
      "          (downsample): Sequential(\n",
      "            (0): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (1): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (2): Flatten(\n",
      "            )\n",
      "            (3): Linear(in_features=6200, out_features=1, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (output): Output(\n",
      "        (res_conv_o): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (3): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (4): BasicBlock(\n",
      "            (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "          (5): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (sampledown): Sequential(\n",
      "      (0): Linear(in_features=896, out_features=300, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=300, out_features=7, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      ")>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "reload() argument must be module",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e4461c395ddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: reload() argument must be module"
     ]
    }
   ],
   "source": [
    "# refresh\n",
    "\n",
    "net, best_acc, config.start_epoch = get_net(config, vocab)\n",
    "\n",
    "reload(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception KeyError: KeyError(<weakref at 0x7f6387622470; to 'tqdm' at 0x7f638c26c710>,) in <bound method tqdm.__del__ of 0it [00:00, ?it/s]> ignored\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Performing basic indexing on a tensor and encountered an error indexing dim 0 with an object of type torch.cuda.LongTensor. The only supported types are integers, slices, numpy scalars, or if indexing with a torch.cuda.LongTensor or torch.cuda.ByteTensor only a single Tensor may be passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7535fca2bb65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-a07ddcd4f8b5>\u001b[0m in \u001b[0;36mtrain_all\u001b[0;34m(net, data, iters, config)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} epoch\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a07ddcd4f8b5>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(net, config, data, train_iter, epoch)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a07ddcd4f8b5>\u001b[0m in \u001b[0;36mrun_net\u001b[0;34m(net, config, data)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# print('t:', topics.size(), type(topics))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/jiwan/tqa/baseline/mc/models/moduleNet/moduleNet.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, CO, Q, A)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# encoding layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/jiwan/tqa/baseline/mc/models/moduleNet/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, CO, Q, A)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# </attention>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# pick top 1 topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mbatch_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/jiwan/p2/local/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mIndexSelect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# else fall through and raise an error in Index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/jiwan/p2/local/lib/python2.7/site-packages/torch/autograd/_functions/tensor.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, index)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_shared_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Performing basic indexing on a tensor and encountered an error indexing dim 0 with an object of type torch.cuda.LongTensor. The only supported types are integers, slices, numpy scalars, or if indexing with a torch.cuda.LongTensor or torch.cuda.ByteTensor only a single Tensor may be passed."
     ]
    }
   ],
   "source": [
    "train_all(net, data, iters, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
